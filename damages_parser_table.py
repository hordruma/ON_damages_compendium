"""
Ontario Damages Compendium Parser - Hybrid Camelot Stream+Lattice + LLM Approach

This parser uses a hybrid approach:
- STREAM mode: Captures section headers (Forearm, General, etc.) from row 0
- LATTICE mode: Extracts clean table structure with bordered cells
- LLM: Parses each row to handle complex cases and multi-plaintiff scenarios

Key features:
- Hybrid section detection: Stream finds sections, lattice parses data
- Row-by-row LLM processing (handles multi-plaintiff cases)
- Pre-labeled columns from table headers
- Deterministic continuation row merging
- Accurate anatomical region tracking

Advantages over full-page extraction:
- 10-50x cheaper (sends only row text, not full pages)
- More accurate section detection (no "General Damages" false positives)
- Better table structure parsing (lattice mode for borders)
- Handles complex cases (multiple plaintiffs in one cell)
- Faster processing (smaller prompts)
- Works with lighter models (gpt-4o-mini, gpt-5-nano)
- Simpler merging logic (no fuzzy matching needed)

Usage:
    from damages_parser_table import parse_compendium_tables

    cases = parse_compendium_tables(
        "2024damagescompendium.pdf",
        endpoint="https://your-resource.openai.azure.com/",
        api_key="your-api-key",
        model="gpt-5-nano"
    )
"""

import json
import time
import re
from pathlib import Path
from typing import List, Dict, Any, Optional, Set, Tuple
import camelot
import requests
from collections import deque


class RateLimiter:
    """Rate limiter to control API requests per minute."""

    def __init__(self, requests_per_minute: int = 200):
        self.requests_per_minute = requests_per_minute
        self.request_times: deque = deque()
        self.window_seconds = 60.0

    def wait_if_needed(self):
        """Wait if necessary to stay within rate limits."""
        now = time.time()

        # Remove requests older than our window
        while self.request_times and self.request_times[0] < now - self.window_seconds:
            self.request_times.popleft()

        # If we're at the limit, wait
        if len(self.request_times) >= self.requests_per_minute:
            oldest_request = self.request_times[0]
            sleep_time = (oldest_request + self.window_seconds) - now
            if sleep_time > 0:
                time.sleep(sleep_time)
                now = time.time()
                while self.request_times and self.request_times[0] < now - self.window_seconds:
                    self.request_times.popleft()

        # Record this request
        self.request_times.append(time.time())


class TableBasedParser:
    """
    Parses damages compendium using table extraction.

    Much more efficient than full-page text extraction:
    - Processes one row at a time
    - Pre-labeled columns
    - Deterministic merging
    """

    # Row parsing prompt - much simpler and cheaper than full-page
    ROW_PROMPT = """Parse this table row from a legal damages compendium.

Body Region/Category: {section}
Table Columns: {columns}
Row Data: {row_data}

Extract the following information and return as JSON:
{{
  "case_name": "Full case name (plaintiff v. defendant)" or null,
  "plaintiff_name": "Plaintiff name only" or null,
  "defendant_name": "Defendant name only" or null,
  "year": year as integer or null,
  "citation": "Citation string" or null,
  "court": "Court name" or null,
  "judge": "Judge's LAST NAME ONLY (e.g., 'Smith' not 'A. Smith J.')" or null,
  "sex": "M" or "F" or null,
  "age": age as integer or null,
  "non_pecuniary_damages": amount in dollars (number, no $ or commas) or null,
  "is_provisional": true/false or null,
  "injuries": ["injury1", "injury2"] or [],
  "other_damages": [{{"type": "future_loss_of_income|past_loss_of_income|cost_of_future_care|housekeeping_capacity|other", "amount": number, "description": "text"}}] or [],
  "comments": "Additional notes" or null,
  "is_continuation": true if this row lacks case_name/citation (continuation of previous case), false otherwise
}}

IMPORTANT:
- Set "is_continuation": true if this row has no case name or citation (it's continuing the previous case)
- Judge names: Extract LAST NAME ONLY, no first initials, no titles (J., J.A., J.J.A., C.J., etc.)
  Examples: "Smith J." -> "Smith", "A. Brown J.A." -> "Brown", "Hon. Jones" -> "Jones"
- Parse monetary amounts as numbers only (no $ or commas)
- Return only valid JSON, no other text

Return the JSON object:"""

    def __init__(
        self,
        endpoint: str,
        api_key: str,
        model: str,
        api_version: str = "2024-02-15-preview",
        verbose: bool = True,
        rate_limiter: Optional[RateLimiter] = None
    ):
        """
        Initialize the table-based parser.

        Args:
            endpoint: Azure endpoint URL
            api_key: Azure API key
            model: Model deployment name
            api_version: Azure API version
            verbose: Whether to print progress
            rate_limiter: Optional rate limiter
        """
        self.endpoint = endpoint.rstrip('/')
        self.api_key = api_key
        self.model = model
        self.api_version = api_version
        self.verbose = verbose
        self.rate_limiter = rate_limiter
        self.errors: List[Dict[str, Any]] = []

        # Detect model type
        self.is_claude = 'claude' in model.lower()
        model_lower = model.lower()
        self.uses_max_completion_tokens = any([
            'gpt-4o' in model_lower,
            'gpt-5' in model_lower,
            'o1-' in model_lower,
            'o3-' in model_lower,
            'chatgpt-4o' in model_lower
        ])

        # Temperature settings
        if any(x in model_lower for x in ['claude-3-5', '3.5-sonnet', '5-sonnet', 'sonnet-3-5', 'nano']):
            self.temperature = 1.0
        else:
            self.temperature = 0.1

    def _call_api(self, prompt: str, max_retries: int = 3) -> Optional[str]:
        """Call Azure API with retry logic."""
        if self.rate_limiter:
            self.rate_limiter.wait_if_needed()

        headers = {
            "Content-Type": "application/json",
            "api-key": self.api_key,
        }

        # Determine URL based on model type
        if self.is_claude:
            url = f"{self.endpoint}/models/{self.model}/chat/completions"
        else:
            url = f"{self.endpoint}/openai/deployments/{self.model}/chat/completions?api-version={self.api_version}"

        payload = {
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature,
        }

        if self.uses_max_completion_tokens:
            payload["max_completion_tokens"] = 2048  # Rows are small, don't need much
        else:
            payload["max_tokens"] = 2048

        for attempt in range(max_retries):
            try:
                response = requests.post(url, json=payload, headers=headers, timeout=60)

                if response.status_code == 200:
                    result = response.json()
                    if "choices" in result and len(result["choices"]) > 0:
                        return result["choices"][0]["message"]["content"]
                    return None

                elif response.status_code == 429:
                    retry_after = response.headers.get('Retry-After')
                    wait_time = int(retry_after) if retry_after and retry_after.isdigit() else 1
                    if self.verbose:
                        print(f"  Rate limit, waiting {wait_time}s...")
                    time.sleep(wait_time)
                    continue

                else:
                    if self.verbose:
                        print(f"  API error {response.status_code}: {response.text}")
                    return None

            except Exception as e:
                if self.verbose:
                    print(f"  Request error (attempt {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(1)
                    continue
                return None

        return None

    @staticmethod
    def normalize_judge_name(judge_name: str) -> str:
        """
        Normalize judge names to last name only.

        Examples:
            "Smith J." -> "Smith"
            "A. Smith J.A." -> "Smith"
            "Hon. John Smith J." -> "Smith"
            "Smith, J." -> "Smith"
            "Brown J.J.A." -> "Brown"

        Args:
            judge_name: Raw judge name

        Returns:
            Normalized last name only
        """
        if not judge_name:
            return ""

        name = judge_name.strip()

        # Remove trailing titles and suffixes (J., J.A., J.J.A., C.J., etc.)
        name = re.sub(r',?\s*(J\.J\.A\.|J\.A\.|J\.|C\.J\.|C\.J\.O\.|C\.J\.C\.)$', '', name, flags=re.IGNORECASE)

        # Remove "The Honourable", "Hon.", etc. at start
        name = re.sub(r'^(The\s+)?(Hon\.?|Honourable)\s+', '', name, flags=re.IGNORECASE)

        # Remove any remaining commas
        name = name.replace(',', '')

        # Standardize spacing
        name = re.sub(r'\s+', ' ', name).strip()

        # Extract last name (last word after splitting)
        # This handles "A. Smith", "John Smith", "A.B. Smith" -> "Smith"
        if name:
            parts = name.split()
            if parts:
                # Last part is the last name
                last_name = parts[-1]
                # Clean up any remaining periods
                last_name = last_name.rstrip('.')
                return last_name

        return ""

    def detect_section_header(self, page_text: str) -> str:
        """
        Detect body region section from page text.

        Looks for common section headers like:
        - HEAD
        - BRAIN & SKULL
        - SPINE
        - CERVICAL SPINE
        - etc.
        """
        if not page_text:
            return "UNKNOWN"

        # Common section patterns (uppercase headings)
        sections = [
            "BRAIN & SKULL", "BRAIN AND SKULL",
            "HEAD",
            "CERVICAL SPINE",
            "THORACIC SPINE",
            "LUMBAR SPINE",
            "SPINE",
            "NECK",
            "SHOULDER",
            "ARM", "ARMS",
            "ELBOW",
            "WRIST", "HAND",
            "CHEST", "THORAX",
            "ABDOMEN",
            "PELVIS",
            "HIP",
            "KNEE",
            "LEG", "LEGS",
            "ANKLE", "FOOT",
            "PSYCHOLOGICAL", "PSYCHIATRIC",
            "MULTIPLE INJURIES",
            "SOFT TISSUE",
        ]

        # Look for section header in first 500 chars
        text_upper = page_text[:500].upper()

        for section in sections:
            # Look for standalone section name (not part of another word)
            pattern = r'\b' + re.escape(section) + r'\b'
            if re.search(pattern, text_upper):
                return section

        return "UNKNOWN"

    def extract_section_from_stream(self, pdf_path: str, page_spec: str) -> Dict[int, Optional[str]]:
        """
        Extract section headers from stream mode row 0.

        Uses stream mode to capture section headers that lattice mode misses.
        Returns a dict mapping page numbers to section headers.

        Args:
            pdf_path: Path to PDF file
            page_spec: Page specification (e.g., "1-10" or "all")

        Returns:
            Dict mapping page number to section header (or None if not found)
        """
        # Known anatomical section keywords
        section_keywords = {
            'General', 'Cervical Spine', 'Thoracic Spine', 'Lumbar Spine',
            'Shoulder', 'Elbow', 'Forearm', 'Wrist', 'Hand', 'Finger',
            'Hip', 'Knee', 'Lower Leg', 'Ankle', 'Foot', 'Toe',
            'Brain', 'Head', 'Face', 'Eye', 'Ear', 'Nose',
            'Psychological', 'Chronic Pain', 'Multiple Injuries'
        }

        sections_by_page = {}

        try:
            # Use stream mode to capture section headers
            tables_stream = camelot.read_pdf(pdf_path, pages=page_spec, flavor="stream")

            for table in tables_stream:
                page_num = table.page
                df_stream = table.df

                if len(df_stream) > 0:
                    # Check row 0 for section keywords
                    row0_values = df_stream.iloc[0].tolist()
                    for cell in row0_values:
                        cell_str = str(cell).strip()
                        if cell_str in section_keywords:
                            sections_by_page[page_num] = cell_str
                            break

                    # If not found, set to None for this page
                    if page_num not in sections_by_page:
                        sections_by_page[page_num] = None
        except Exception as e:
            if self.verbose:
                print(f"  Stream mode section extraction warning: {e}")

        return sections_by_page

    def extract_tables_from_pdf(self, pdf_path: str, page_spec: str) -> List[Any]:
        """
        Extract tables from PDF using Camelot.

        Args:
            pdf_path: Path to PDF file
            page_spec: Page specification (e.g., "1-10" or "all")

        Returns:
            List of Camelot table objects
        """
        try:
            tables = camelot.read_pdf(
                pdf_path,
                pages=page_spec,
                flavor="lattice"
                # Don't strip newlines - we need them for header detection
            )
            return tables if tables else []
        except Exception as e:
            if self.verbose:
                print(f"  Table extraction error: {e}")
            return []

    def parse_row(
        self,
        row: List[str],
        columns: List[str],
        section: str,
        page_number: int
    ) -> Optional[Dict[str, Any]]:
        """
        Parse a single table row.

        Args:
            row: List of cell values
            columns: List of column headers
            section: Body region/section name
            page_number: Page number (for logging)

        Returns:
            Parsed row data or None if parsing fails
        """
        # Format row data for prompt
        row_data = []
        for i, (col, val) in enumerate(zip(columns, row)):
            if val and val.strip():
                row_data.append(f"{col}: {val.strip()}")

        if not row_data:
            return None

        row_data_str = "\n".join(row_data)
        columns_str = ", ".join(columns)

        prompt = self.ROW_PROMPT.format(
            section=section,
            columns=columns_str,
            row_data=row_data_str
        )

        response = self._call_api(prompt)

        if response:
            # Extract JSON from response
            json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response, re.DOTALL)
            if json_match:
                response = json_match.group(1)

            try:
                data = json.loads(response)
                data['source_page'] = page_number
                data['category'] = section

                # Normalize judge name to last name only
                if data.get('judge'):
                    data['judge'] = self.normalize_judge_name(data['judge'])

                return data
            except json.JSONDecodeError as e:
                if self.verbose:
                    print(f"  JSON parse error: {e}")
                return None

        return None

    def merge_continuation_row(self, case: Dict[str, Any], row_data: Dict[str, Any]) -> None:
        """
        Merge a continuation row into an existing case.

        Continuation rows lack case name/citation but have additional
        damages, injuries, or comments.
        """
        # Merge injuries
        if row_data.get('injuries'):
            existing_injuries = set(case.get('injuries', []))
            existing_injuries.update(row_data['injuries'])
            case['injuries'] = list(existing_injuries)

        # Merge other_damages
        if row_data.get('other_damages'):
            case.setdefault('other_damages', []).extend(row_data['other_damages'])

        # Append comments
        if row_data.get('comments'):
            existing_comments = case.get('comments', '')
            if existing_comments:
                case['comments'] = f"{existing_comments} | {row_data['comments']}"
            else:
                case['comments'] = row_data['comments']

        # Update damages if higher
        new_npd = row_data.get('non_pecuniary_damages')
        if new_npd is not None:
            existing_npd = case.get('non_pecuniary_damages')
            if existing_npd is None or new_npd > existing_npd:
                case['non_pecuniary_damages'] = new_npd

    def parse_pdf(
        self,
        pdf_path: str,
        start_page: int = 1,
        end_page: Optional[int] = None,
        output_json: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Parse PDF using Camelot table extraction + LLM row parsing.

        Args:
            pdf_path: Path to PDF
            start_page: Starting page (1-indexed)
            end_page: Ending page (None = all)
            output_json: Optional path to save results

        Returns:
            List of parsed cases
        """
        all_cases = []
        current_case = None
        total_rows = 0
        continuation_rows = 0

        # Build page specification for Camelot
        if end_page is None:
            page_spec = "all"
        else:
            page_spec = f"{start_page}-{end_page}"

        if self.verbose:
            print(f"Parsing pages {page_spec}")
            print(f"Using Camelot table extraction + LLM row parsing")
            print(f"Model: {self.model}")

        # HYBRID APPROACH: Extract sections using stream mode
        if self.verbose:
            print("\nðŸ“„ Extracting section headers with stream mode...")

        sections_from_stream = self.extract_section_from_stream(pdf_path, page_spec)

        if self.verbose:
            found_count = sum(1 for s in sections_from_stream.values() if s)
            print(f"âœ… Found {found_count} section headers from stream mode")

        # Extract all tables using lattice mode
        if self.verbose:
            print("\nðŸ“„ Extracting tables with lattice mode...")

        tables = self.extract_tables_from_pdf(pdf_path, page_spec)

        if self.verbose:
            print(f"âœ… Extracted {len(tables)} tables from lattice mode")

        # Track current section per page
        section_by_page = {}

        # Process each table
        for table_idx, table in enumerate(tables):
            page_number = table.page  # Camelot table objects have .page attribute

            if self.verbose and (table_idx == 0 or table.page != tables[table_idx-1].page):
                print(f"\nPage {page_number}...", end=" ")

            # Use section from stream mode, fallback to table detection
            if page_number not in section_by_page:
                # Try stream mode section first
                section = sections_from_stream.get(page_number)

                # Fallback to table content detection if stream didn't find it
                if not section:
                    section = self.detect_section_from_table(table)

                # Store uppercase version for consistency
                section_by_page[page_number] = section.upper() if section else "UNKNOWN"

            section = section_by_page[page_number]

            # Get table data as DataFrame
            df = table.df

            if len(df) < 2:  # Need at least header + data
                continue

            # STRUCTURE DETECTION (from test notebook that worked)
            # Type 1: Headers spread across row 0 (pages 91-95) - data starts row 1
            # Type 2: Newline-separated headers in row 0 - data starts row 1
            # Type 3: Section in row 0, headers in row 1 (pages 21-25) - data starts row 2

            row0_cell0 = str(df.iloc[0, 0]).strip() if len(df) > 0 else ""
            row0_values = [str(cell).strip() for cell in df.iloc[0].tolist()] if len(df) > 0 else []
            num_filled_cells = sum(1 for v in row0_values if v and v != 'nan')

            header = []
            data_start_row = 1

            if num_filled_cells > 1:
                # Type 1: Headers spread across columns (pages 91-95)
                header = [v if v and v != 'nan' else f"Col_{i}" for i, v in enumerate(row0_values)]
                data_start_row = 1

            elif '\n' in row0_cell0 or '\\n' in row0_cell0:
                # Type 2: Newline-separated headers in row 0, col 0
                headers_raw = row0_cell0.replace('\\n', '\n').split('\n')
                header = [h.strip() for h in headers_raw if h.strip()]
                data_start_row = 1

            else:
                # Type 3: Section in row 0, headers in row 1 (pages 21-25)
                if len(df) > 1:
                    row1_cell0 = str(df.iloc[1, 0]).strip()
                    row1_values = [str(cell).strip() for cell in df.iloc[1].tolist()]
                    num_filled_row1 = sum(1 for v in row1_values if v and v != 'nan')

                    if '\n' in row1_cell0 or '\\n' in row1_cell0:
                        # Headers newline-separated in row 1
                        headers_raw = row1_cell0.replace('\\n', '\n').split('\n')
                        header = [h.strip() for h in headers_raw if h.strip()]
                    elif num_filled_row1 > 1:
                        # Headers spread across row 1
                        header = [v if v and v != 'nan' else f"Col_{i}" for i, v in enumerate(row1_values)]
                    else:
                        header = [str(h).strip() for h in df.iloc[1].tolist() if str(h).strip()]

                    data_start_row = 2
                else:
                    continue  # Not enough rows

            # Validate headers
            if not header or not any(h.lower() in ['plaintiff', 'case', 'year', 'defendant'] for h in header):
                if self.verbose:
                    print(f"SKIP - headers: {header[:5] if header else 'None'}")
                continue

            if self.verbose:
                print(f"Headers: {header[:5] if len(header) > 5 else header}, data_start: {data_start_row}, df_len: {len(df)}")

            page_rows = 0
            page_new = 0
            page_merged = 0

            # Process data rows starting from correct row
            for idx in range(data_start_row, len(df)):
                row = df.iloc[idx]
                row_cells = [str(cell).strip() if cell else "" for cell in row]

                # Skip empty rows
                if not any(row_cells):
                    continue

                page_rows += 1
                total_rows += 1

                # Parse row with LLM
                row_data = self.parse_row(row_cells, header, section, page_number)

                if not row_data:
                    continue

                # Check if continuation row
                if row_data.get('is_continuation') and current_case:
                    # Merge into current case
                    self.merge_continuation_row(current_case, row_data)
                    page_merged += 1
                    continuation_rows += 1
                else:
                    # New case
                    if current_case:
                        all_cases.append(current_case)

                    current_case = row_data
                    page_new += 1

            if self.verbose and page_rows > 0:
                print(f"{page_rows} rows, {page_new} new, {page_merged} merged")

            # Save incremental results every 10 tables
            if output_json and table_idx > 0 and table_idx % 10 == 0:
                if current_case:
                    all_cases.append(current_case)
                    current_case = None
                with open(output_json, 'w') as f:
                    json.dump(all_cases, f, indent=2)

        # Add final case
        if current_case:
            all_cases.append(current_case)

        if self.verbose:
            print(f"\nâœ“ Parsing complete")
            print(f"  Total rows processed: {total_rows}")
            print(f"  Continuation rows merged: {continuation_rows}")
            print(f"  Unique cases: {len(all_cases)}")

        # Save final results
        if output_json:
            with open(output_json, 'w') as f:
                json.dump(all_cases, f, indent=2)

        return all_cases

    def detect_section_from_table(self, table) -> str:
        """
        Detect body region section from table content.

        Since we're using Camelot, we extract section from table text.
        """
        # Get table text
        df = table.df
        table_text = " ".join([str(cell) for row in df.values for cell in row])

        return self.detect_section_header(table_text)


def parse_compendium_tables(
    pdf_path: str,
    endpoint: str,
    api_key: str,
    model: str,
    output_json: str = "damages_table_based.json",
    start_page: Optional[int] = None,
    end_page: Optional[int] = None,
    verbose: bool = True,
    requests_per_minute: int = 200
) -> List[Dict[str, Any]]:
    """
    Parse Ontario Damages Compendium using hybrid Camelot + LLM approach.

    This is more efficient than full-page parsing:
    - Better table detection (Camelot lattice-based)
    - Handles complex cases (multiple plaintiffs per cell)
    - 10-50x cheaper (processes rows instead of full pages)
    - Faster (smaller prompts)
    - More reliable (structured input)
    - Works well with lighter models

    Args:
        pdf_path: Path to PDF
        endpoint: Azure endpoint
        api_key: Azure API key
        model: Model deployment name (gpt-5-nano works great!)
        output_json: Path to save results
        start_page: Starting page
        end_page: Ending page
        verbose: Print progress
        requests_per_minute: Rate limit

    Returns:
        List of parsed cases

    Example:
        cases = parse_compendium_tables(
            "2024damagescompendium.pdf",
            endpoint="https://your-resource.openai.azure.com/",
            api_key="your-key",
            model="gpt-5-nano"  # Cheaper model works fine!
        )
    """
    rate_limiter = RateLimiter(requests_per_minute) if requests_per_minute > 0 else None

    if verbose and rate_limiter:
        print(f"Rate limiting: {requests_per_minute} requests/minute")

    parser = TableBasedParser(
        endpoint=endpoint,
        api_key=api_key,
        model=model,
        verbose=verbose,
        rate_limiter=rate_limiter
    )

    return parser.parse_pdf(
        pdf_path=pdf_path,
        start_page=start_page or 1,
        end_page=end_page,
        output_json=output_json
    )


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 5:
        print("Usage: python damages_parser_table.py <pdf_path> <endpoint> <api_key> <model> [output_json]")
        print("\nExample:")
        print('  python damages_parser_table.py 2024damagescompendium.pdf \\')
        print('    "https://your-resource.openai.azure.com/" \\')
        print('    "your-api-key" \\')
        print('    "gpt-5-nano" \\')  # Note: cheaper model!
        print('    "damages_table_based.json"')
        sys.exit(1)

    pdf_path = sys.argv[1]
    endpoint = sys.argv[2]
    api_key = sys.argv[3]
    model = sys.argv[4]
    output_json = sys.argv[5] if len(sys.argv) > 5 else "damages_table_based.json"

    print(f"Parsing {pdf_path} using table extraction...")
    cases = parse_compendium_tables(pdf_path, endpoint, api_key, model, output_json)
    print(f"\nDone! Parsed {len(cases)} cases")
    print(f"Saved to {output_json}")
