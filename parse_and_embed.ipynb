{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontario Damages Compendium - Hybrid Camelot + LLM Extraction\n",
    "\n",
    "This notebook uses a hybrid approach:\n",
    "1. **Camelot** extracts tables from PDF (better table detection)\n",
    "2. **LLM** parses each row (handles multi-plaintiff cases and complex data)\n",
    "\n",
    "This approach combines the best of both worlds!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports complete\n"
     ]
    }
   ],
   "source": [
    "from damages_parser_table import parse_compendium_tables\n",
    "from data_transformer import add_embeddings_to_cases\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configuration\n",
    "PDF_PATH = \"2024damagescompendium.pdf\"\n",
    "OUTPUT_JSON = \"damages_table_based.json\"\n",
    "DASHBOARD_JSON = \"data/damages_with_embeddings.json\"\n",
    "\n",
    "# Azure Configuration (fill these in)\n",
    "ENDPOINT = os.getenv(\"ENDPOINT\")\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "MODEL = os.getenv(\"MODEL\")\n",
    "\n",
    "# Create data directory\n",
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Table Extraction (Small Sample)\n",
    "\n",
    "Let's first test on a small page range to verify the approach works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limiting: 200 requests/minute\n",
      "Parsing pages 1-5\n",
      "Using Camelot table extraction + LLM row parsing\n",
      "Model: gpt-5-chat\n",
      "\n",
      "üìÑ Extracting section headers with stream mode...\n",
      "‚úÖ Found 2 section headers from stream mode\n",
      "\n",
      "üìÑ Extracting tables with lattice mode...\n",
      "‚úÖ Extracted 3 tables from lattice mode\n",
      "\n",
      "Page 1... SKIP - headers: ['COMPENDIUM OF DAMAGES AWARDED IN', 'PERSONAL INJURY ACTIONS ACROSS ONTARIO', 'JANUARY 1999 - OCTOBER 2024', 'THE HONOURABLE JUSTICE JAYE HOOPER', 'AND THE HONOURABLE JAMES B. CHADWICK, Q.C.']\n",
      "\n",
      "Page 4... \n",
      "DEBUG Type 3:\n",
      "  row1_cell0: 'Plaintiff'\n",
      "  num_filled_row1: 10\n",
      "  row1_values: ['Plaintiff', 'Defendant', 'Year']\n",
      "Headers: ['Plaintiff', 'Defendant', 'Year', 'Citation', 'Court'], data_start: 2, df_len: 6\n",
      "4 rows, 4 new, 0 merged\n",
      "\n",
      "Page 5... \n",
      "DEBUG Type 3:\n",
      "  row1_cell0: 'Plaintiff \\nDefendant \\nYear \\nCitation \\nCourt \\nJudge \\nSex \\nNon-Pecuniary \\nOther Damages \\nComments \\nAge'\n",
      "  num_filled_row1: 1\n",
      "  row1_values: ['Plaintiff \\nDefendant \\nYear \\nCitation \\nCourt \\nJudge \\nSex \\nNon-Pecuniary \\nOther Damages \\nComments \\nAge \\nGeneral \\nDamages', '', '']\n",
      "Headers: ['Plaintiff', 'Defendant', 'Year', 'Citation', 'Court'], data_start: 2, df_len: 4\n",
      "2 rows, 2 new, 0 merged\n",
      "\n",
      "‚úì Parsing complete\n",
      "  Total rows processed: 6\n",
      "  Continuation rows merged: 0\n",
      "  Unique cases: 6\n",
      "\n",
      "‚úÖ Test complete: 6 cases extracted\n",
      "\n",
      "Sample case:\n",
      "{\n",
      "  \"case_name\": \"Dusk v. Malone\",\n",
      "  \"plaintiff_name\": \"Dusk\",\n",
      "  \"defendant_name\": \"Malone\",\n",
      "  \"year\": 1999,\n",
      "  \"citation\": \"[1999] O.J. No. 3917; [2003] O.J. No. 44; (2003) 167 O.A.C. 333\",\n",
      "  \"court\": \"Ontario Court of Appeal affirming Superior Court of Justice (Toronto, Ontario)\",\n",
      "  \"judge\": [\n",
      "    \"Brennan\",\n",
      "    \"O\\u2019Connor\",\n",
      "    \"Catzman\",\n",
      "    \"Weiler\"\n",
      "  ],\n",
      "  \"sex\": \"M\",\n",
      "  \"age\": 2,\n",
      "  \"non_pecuniary_damages\": 75000,\n",
      "  \"is_provisional\": false,\n",
      "  \"injuries\": [\n",
      "    \"tightness in parts of neck\",\n",
      "    \"pain in chest and back\",\n",
      "    \"restricted neck movement\",\n",
      "    \"facial injury requiring plastic surgery\",\n",
      "    \"broken teeth\",\n",
      "    \"knee lacerations requiring sutures\",\n",
      "    \"whiplash to cervical and lumbar spine\",\n",
      "    \"degenerative disc disease\",\n",
      "    \"numbness and tingling of hands\",\n",
      "    \"spasms\",\n",
      "    \"anxiety\",\n",
      "    \"loss of confidence\",\n",
      "    \"memory loss\"\n",
      "  ],\n",
      "  \"other_damages\": [\n",
      "    {\n",
      "      \"type\": \"other\",\n",
      "      \"amount\": 5000,\n",
      "      \"description\": \"Family Law Act claim (wife)\"\n",
      "    }\n",
      "  ],\n",
      "  \"comments\": \"85% improved 3 years after accident. Largely recovered 11 years later.\",\n",
      "  \"is_continuation\": false,\n",
      "  \"source_page\": 4,\n",
      "  \"category\": \"GENERAL\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test on just 5 pages first\n",
    "test_cases = parse_compendium_tables(\n",
    "    PDF_PATH,\n",
    "    endpoint=ENDPOINT,\n",
    "    api_key=API_KEY,\n",
    "    model=MODEL,\n",
    "    output_json=\"test_output.json\",\n",
    "    start_page=1,\n",
    "    end_page=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Test complete: {len(test_cases)} cases extracted\")\n",
    "\n",
    "# View a sample case\n",
    "if test_cases:\n",
    "    print(\"\\nSample case:\")\n",
    "    print(json.dumps(test_cases[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parse Full PDF\n",
    "\n",
    "Once the test looks good, parse the entire PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limiting: 200 requests/minute\n",
      "Parsing pages all\n",
      "Using Camelot table extraction + LLM row parsing\n",
      "Model: gpt-5-chat\n",
      "\n",
      "üìÑ Extracting section headers with stream mode...\n"
     ]
    }
   ],
   "source": [
    "# Parse full PDF\n",
    "cases = parse_compendium_tables(\n",
    "    PDF_PATH,\n",
    "    endpoint=ENDPOINT,\n",
    "    api_key=API_KEY,\n",
    "    model=MODEL,\n",
    "    output_json=OUTPUT_JSON,\n",
    "    verbose=True,\n",
    "    requests_per_minute=200  # Azure rate limit\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Parsed {len(cases)} cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parse Specific Page Range\n",
    "\n",
    "Or parse specific pages if you want to resume or test a section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Parse specific range\n",
    "cases = parse_compendium_tables(\n",
    "    PDF_PATH,\n",
    "    endpoint=ENDPOINT,\n",
    "    api_key=API_KEY,\n",
    "    model=MODEL,\n",
    "    output_json=OUTPUT_JSON,\n",
    "    start_page=10,\n",
    "    end_page=50,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Parsed pages 10-50: {len(cases)} cases\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Embeddings for Dashboard\n",
    "\n",
    "Convert parsed cases to dashboard format with embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dashboard format and generate embeddings\n",
    "dashboard_cases = add_embeddings_to_cases(\n",
    "    OUTPUT_JSON,\n",
    "    DASHBOARD_JSON\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(dashboard_cases)} dashboard-ready cases\")\n",
    "print(f\"\\nüìÅ Saved to:\")\n",
    "print(f\"  - Raw parsed: {OUTPUT_JSON}\")\n",
    "print(f\"  - Dashboard: {DASHBOARD_JSON}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze\n",
    "with open(OUTPUT_JSON) as f:\n",
    "    cases = json.load(f)\n",
    "\n",
    "print(f\"üìä Statistics:\")\n",
    "print(f\"  Total cases: {len(cases):,}\")\n",
    "\n",
    "# Count multi-plaintiff cases\n",
    "multi_plaintiff = sum(1 for c in cases if len(c.get('plaintiffs', [])) > 1)\n",
    "print(f\"  Multi-plaintiff cases: {multi_plaintiff:,}\")\n",
    "\n",
    "# Count cases with damages\n",
    "with_damages = sum(1 for c in cases if c.get('non_pecuniary_damages'))\n",
    "print(f\"  Cases with damages: {with_damages:,}\")\n",
    "\n",
    "# Count by category\n",
    "categories = {}\n",
    "for c in cases:\n",
    "    cat = c.get('category', 'UNKNOWN')\n",
    "    categories[cat] = categories.get(cat, 0) + 1\n",
    "\n",
    "print(\"\\nüè• Top categories:\")\n",
    "for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"  {cat}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View Sample Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample cases\n",
    "print(\"\\nüìã Sample Cases:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, case in enumerate(cases[:3], 1):\n",
    "    print(f\"\\nCase {i}:\")\n",
    "    print(f\"  Case Name: {case.get('case_name')}\")\n",
    "    print(f\"  Category: {case.get('category')}\")\n",
    "    print(f\"  Year: {case.get('year')}\")\n",
    "    print(f\"  Court: {case.get('court')}\")\n",
    "    print(f\"  Judge: {case.get('judge')}\")\n",
    "    \n",
    "    if case.get('plaintiffs'):\n",
    "        print(f\"  Plaintiffs: {len(case['plaintiffs'])}\")\n",
    "        for p in case['plaintiffs']:\n",
    "            print(f\"    - {p.get('plaintiff_id')}: ${p.get('non_pecuniary_damages'):,}\" \n",
    "                  if p.get('non_pecuniary_damages') else f\"    - {p.get('plaintiff_id')}\")\n",
    "    \n",
    "    if case.get('injuries'):\n",
    "        print(f\"  Injuries: {', '.join(case['injuries'][:3])}\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run the Streamlit app: `streamlit run streamlit_app.py`\n",
    "2. Test the search functionality with various injury descriptions\n",
    "3. Verify that multi-plaintiff cases are handled correctly\n",
    "\n",
    "## Why This Approach Works Better\n",
    "\n",
    "**Camelot Table Extraction:**\n",
    "- Better at detecting table boundaries\n",
    "- Handles complex table layouts\n",
    "- More reliable than pdfplumber for structured tables\n",
    "\n",
    "**LLM Row Parsing:**\n",
    "- Handles multiple plaintiffs in one cell\n",
    "- Extracts complex damage breakdowns\n",
    "- Normalizes judge names\n",
    "- Detects continuation rows\n",
    "\n",
    "**Cost Effective:**\n",
    "- Only sends row text to LLM (not full pages)\n",
    "- 10-50x cheaper than full-page approaches\n",
    "- Works well with lighter models (gpt-5-nano, 4o-mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "\n",
    "# Load the dashboard cases with embeddings\n",
    "with open(DASHBOARD_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    cases = json.load(f)\n",
    "\n",
    "# Use the same embedding model\n",
    "emb_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Build injury-focused search_text and embeddings\n",
    "ids = []\n",
    "inj_embs = []\n",
    "out_cases = []\n",
    "\n",
    "for c in tqdm.tqdm(cases, desc=\"Generate injury-focused embeddings\"):\n",
    "    # Build search_text from injuries + sequelae only\n",
    "    ext = c.get(\"extended_data\", {}) or {}\n",
    "    injuries = ext.get(\"injuries\") or []\n",
    "    \n",
    "    # join injuries into concise search text\n",
    "    search_text = \"; \".join(injuries) if injuries else \"\"\n",
    "    \n",
    "    # fallback if no injuries\n",
    "    if not search_text:\n",
    "        case_name = c.get(\"case_name\", \"\")\n",
    "        if case_name:\n",
    "            search_text = case_name\n",
    "        else:\n",
    "            search_text = \"case\"\n",
    "    \n",
    "    c['search_text'] = search_text\n",
    "    \n",
    "    # Compute embedding\n",
    "    emb = emb_model.encode(search_text).astype(\"float32\")\n",
    "    c['inj_emb'] = emb.tolist()\n",
    "    \n",
    "    ids.append(c['id'])\n",
    "    inj_embs.append(emb)\n",
    "    out_cases.append(c)\n",
    "\n",
    "# Save artifacts for RAG search\n",
    "data_dir = Path(\"data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save cases with search_text and embeddings\n",
    "with open(data_dir / \"compendium_inj.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(out_cases, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save embedding matrix for fast load\n",
    "emb_matrix = np.vstack(inj_embs)\n",
    "np.save(data_dir / \"embeddings_inj.npy\", emb_matrix)\n",
    "\n",
    "# Save case IDs for mapping\n",
    "with open(data_dir / \"ids.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ids, f)\n",
    "\n",
    "print(f\"‚úÖ Created {len(out_cases)} injury-focused embeddings\")\n",
    "print(f\"   - compendium_inj.json: {(data_dir / 'compendium_inj.json').stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "print(f\"   - embeddings_inj.npy: {(data_dir / 'embeddings_inj.npy').stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "print(f\"   - ids.json: {(data_dir / 'ids.json').stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Injury-Focused Embeddings for RAG Search\n",
    "\n",
    "Create embeddings for semantic search focused only on injuries and sequelae:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
