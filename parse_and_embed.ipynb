{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontario Damages Compendium - Hybrid Camelot + LLM Extraction\n",
    "\n",
    "This notebook uses a hybrid approach:\n",
    "1. **Camelot** extracts tables from PDF (better table detection)\n",
    "2. **LLM** parses each row (handles multi-plaintiff cases and complex data)\n",
    "\n",
    "This approach combines the best of both worlds!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from damages_parser_table import parse_compendium_tables\nfrom data_transformer import add_embeddings_to_cases\nimport json\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configuration\nPDF_PATH = \"2024damagescompendium.pdf\"\nOUTPUT_JSON = \"damages_table_based.json\"\nDASHBOARD_JSON = \"data/damages_with_embeddings.json\"\n\n# Azure Configuration (fill these in)\nENDPOINT = \"https://your-resource.openai.azure.com/\"\nAPI_KEY = \"your-api-key\"\nMODEL = \"gpt-5-nano\"  # or gpt-4o, claude-3-5-sonnet, etc.\n\n# Create data directory\nPath(\"data\").mkdir(exist_ok=True)\n\nprint(\"‚úÖ Imports complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Table Extraction (Small Sample)\n",
    "\n",
    "Let's first test on a small page range to verify the approach works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on just 5 pages first\n",
    "test_cases = parse_compendium_tables(\n",
    "    PDF_PATH,\n",
    "    endpoint=ENDPOINT,\n",
    "    api_key=API_KEY,\n",
    "    model=MODEL,\n",
    "    output_json=\"test_output.json\",\n",
    "    start_page=1,\n",
    "    end_page=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Test complete: {len(test_cases)} cases extracted\")\n",
    "\n",
    "# View a sample case\n",
    "if test_cases:\n",
    "    print(\"\\nSample case:\")\n",
    "    print(json.dumps(test_cases[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parse Full PDF\n",
    "\n",
    "Once the test looks good, parse the entire PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse full PDF\n",
    "cases = parse_compendium_tables(\n",
    "    PDF_PATH,\n",
    "    endpoint=ENDPOINT,\n",
    "    api_key=API_KEY,\n",
    "    model=MODEL,\n",
    "    output_json=OUTPUT_JSON,\n",
    "    verbose=True,\n",
    "    requests_per_minute=200  # Azure rate limit\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Parsed {len(cases)} cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parse Specific Page Range\n",
    "\n",
    "Or parse specific pages if you want to resume or test a section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse specific range\n",
    "cases = parse_compendium_tables(\n",
    "    PDF_PATH,\n",
    "    endpoint=ENDPOINT,\n",
    "    api_key=API_KEY,\n",
    "    model=MODEL,\n",
    "    output_json=OUTPUT_JSON,\n",
    "    start_page=10,\n",
    "    end_page=50,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Parsed pages 10-50: {len(cases)} cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Embeddings for Dashboard\n",
    "\n",
    "Convert parsed cases to dashboard format with embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dashboard format and generate embeddings\n",
    "dashboard_cases = add_embeddings_to_cases(\n",
    "    OUTPUT_JSON,\n",
    "    DASHBOARD_JSON\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(dashboard_cases)} dashboard-ready cases\")\n",
    "print(f\"\\nüìÅ Saved to:\")\n",
    "print(f\"  - Raw parsed: {OUTPUT_JSON}\")\n",
    "print(f\"  - Dashboard: {DASHBOARD_JSON}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze\n",
    "with open(OUTPUT_JSON) as f:\n",
    "    cases = json.load(f)\n",
    "\n",
    "print(f\"üìä Statistics:\")\n",
    "print(f\"  Total cases: {len(cases):,}\")\n",
    "\n",
    "# Count multi-plaintiff cases\n",
    "multi_plaintiff = sum(1 for c in cases if len(c.get('plaintiffs', [])) > 1)\n",
    "print(f\"  Multi-plaintiff cases: {multi_plaintiff:,}\")\n",
    "\n",
    "# Count cases with damages\n",
    "with_damages = sum(1 for c in cases if c.get('non_pecuniary_damages'))\n",
    "print(f\"  Cases with damages: {with_damages:,}\")\n",
    "\n",
    "# Count by category\n",
    "categories = {}\n",
    "for c in cases:\n",
    "    cat = c.get('category', 'UNKNOWN')\n",
    "    categories[cat] = categories.get(cat, 0) + 1\n",
    "\n",
    "print(\"\\nüè• Top categories:\")\n",
    "for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"  {cat}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View Sample Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample cases\n",
    "print(\"\\nüìã Sample Cases:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, case in enumerate(cases[:3], 1):\n",
    "    print(f\"\\nCase {i}:\")\n",
    "    print(f\"  Case Name: {case.get('case_name')}\")\n",
    "    print(f\"  Category: {case.get('category')}\")\n",
    "    print(f\"  Year: {case.get('year')}\")\n",
    "    print(f\"  Court: {case.get('court')}\")\n",
    "    print(f\"  Judge: {case.get('judge')}\")\n",
    "    \n",
    "    if case.get('plaintiffs'):\n",
    "        print(f\"  Plaintiffs: {len(case['plaintiffs'])}\")\n",
    "        for p in case['plaintiffs']:\n",
    "            print(f\"    - {p.get('plaintiff_id')}: ${p.get('non_pecuniary_damages'):,}\" \n",
    "                  if p.get('non_pecuniary_damages') else f\"    - {p.get('plaintiff_id')}\")\n",
    "    \n",
    "    if case.get('injuries'):\n",
    "        print(f\"  Injuries: {', '.join(case['injuries'][:3])}\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run the Streamlit app: `streamlit run streamlit_app.py`\n",
    "2. Test the search functionality with various injury descriptions\n",
    "3. Verify that multi-plaintiff cases are handled correctly\n",
    "\n",
    "## Why This Approach Works Better\n",
    "\n",
    "**Camelot Table Extraction:**\n",
    "- Better at detecting table boundaries\n",
    "- Handles complex table layouts\n",
    "- More reliable than pdfplumber for structured tables\n",
    "\n",
    "**LLM Row Parsing:**\n",
    "- Handles multiple plaintiffs in one cell\n",
    "- Extracts complex damage breakdowns\n",
    "- Normalizes judge names\n",
    "- Detects continuation rows\n",
    "\n",
    "**Cost Effective:**\n",
    "- Only sends row text to LLM (not full pages)\n",
    "- 10-50x cheaper than full-page approaches\n",
    "- Works well with lighter models (gpt-5-nano, 4o-mini)"
   ]
  },
  {
   "cell_type": "code",
   "source": "from sentence_transformers import SentenceTransformer\nimport numpy as np\nimport json\nfrom pathlib import Path\nimport tqdm\n\n# Load the dashboard cases with embeddings\nwith open(DASHBOARD_JSON, \"r\", encoding=\"utf-8\") as f:\n    cases = json.load(f)\n\n# Use the same embedding model\nemb_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Build injury-focused search_text and embeddings\nids = []\ninj_embs = []\nout_cases = []\n\nfor c in tqdm.tqdm(cases, desc=\"Generate injury-focused embeddings\"):\n    # Build search_text from injuries + sequelae only\n    ext = c.get(\"extended_data\", {}) or {}\n    injuries = ext.get(\"injuries\") or []\n    \n    # join injuries into concise search text\n    search_text = \"; \".join(injuries) if injuries else \"\"\n    \n    # fallback if no injuries\n    if not search_text:\n        case_name = c.get(\"case_name\", \"\")\n        if case_name:\n            search_text = case_name\n        else:\n            search_text = \"case\"\n    \n    c['search_text'] = search_text\n    \n    # Compute embedding\n    emb = emb_model.encode(search_text).astype(\"float32\")\n    c['inj_emb'] = emb.tolist()\n    \n    ids.append(c['id'])\n    inj_embs.append(emb)\n    out_cases.append(c)\n\n# Save artifacts for RAG search\ndata_dir = Path(\"data\")\ndata_dir.mkdir(exist_ok=True)\n\n# Save cases with search_text and embeddings\nwith open(data_dir / \"compendium_inj.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(out_cases, f, ensure_ascii=False, indent=2)\n\n# Save embedding matrix for fast load\nemb_matrix = np.vstack(inj_embs)\nnp.save(data_dir / \"embeddings_inj.npy\", emb_matrix)\n\n# Save case IDs for mapping\nwith open(data_dir / \"ids.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(ids, f)\n\nprint(f\"‚úÖ Created {len(out_cases)} injury-focused embeddings\")\nprint(f\"   - compendium_inj.json: {(data_dir / 'compendium_inj.json').stat().st_size / 1024 / 1024:.1f} MB\")\nprint(f\"   - embeddings_inj.npy: {(data_dir / 'embeddings_inj.npy').stat().st_size / 1024 / 1024:.1f} MB\")\nprint(f\"   - ids.json: {(data_dir / 'ids.json').stat().st_size / 1024:.1f} KB\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Generate Injury-Focused Embeddings for RAG Search\n\nCreate embeddings for semantic search focused only on injuries and sequelae:",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}