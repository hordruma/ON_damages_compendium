{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontario Damages Compendium - Data Extraction & Embedding\n",
    "\n",
    "This notebook extracts case data from the 2024 Damages Compendium PDF and generates embeddings for semantic similarity search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "PDF_PATH = \"2024damagescompendium.pdf\"\n",
    "OUTPUT_JSON = \"data/damages_with_embeddings.json\"\n",
    "RAW_CSV = \"data/damages_raw.csv\"\n",
    "\n",
    "# Create data directory\n",
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"âœ… Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Embedding Model\n",
    "\n",
    "We use `sentence-transformers/all-MiniLM-L6-v2` - a lightweight but effective model for semantic similarity.\n",
    "- 384-dimensional embeddings\n",
    "- Fast inference\n",
    "- Good balance of speed and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“¥ Loading embedding model...\")\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(\"âœ… Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Tables from PDF\n",
    "\n",
    "Extract all tables using Camelot with the 'lattice' flavor (best for tables with clear borders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“„ Extracting tables from PDF...\")\n",
    "print(f\"   PDF: {PDF_PATH}\")\n",
    "\n",
    "# Extract tables\n",
    "tables = camelot.read_pdf(\n",
    "    PDF_PATH,\n",
    "    pages=\"all\",\n",
    "    flavor=\"lattice\",\n",
    "    strip_text=\"\\n\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… Extracted {len(tables)} tables\")\n",
    "\n",
    "# Combine all tables\n",
    "raw_frames = [t.df for t in tables]\n",
    "raw = pd.concat(raw_frames, ignore_index=True)\n",
    "\n",
    "print(f\"   Total rows: {len(raw)}\")\n",
    "\n",
    "# Save raw extraction\n",
    "raw.to_csv(RAW_CSV, index=False)\n",
    "print(f\"   Saved raw data to {RAW_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean and Normalize Data\n",
    "\n",
    "Remove empty rows, duplicate headers, and normalize the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ§½ Cleaning and normalizing data...\")\n",
    "\n",
    "# Remove empty rows\n",
    "raw = raw[raw.apply(lambda row: not all(str(v).strip() == \"\" for v in row), axis=1)]\n",
    "\n",
    "# Remove duplicate rows\n",
    "raw = raw.drop_duplicates()\n",
    "\n",
    "print(f\"âœ… Cleaned data: {len(raw)} rows remaining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parse Cases and Extract Structured Data\n",
    "\n",
    "Parse each row into structured case data, detecting region headers and case details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_currency(value):\n",
    "    \"\"\"Extract numeric value from currency string\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    try:\n",
    "        value = str(value).replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
    "        return float(value)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def is_section_header(row):\n",
    "    \"\"\"Check if row is a section header (body region)\"\"\"\n",
    "    row_str = \" \".join([str(v).strip() for v in row if str(v).strip()])\n",
    "    # Section headers are typically all caps and relatively short\n",
    "    return len(row_str) < 100 and row_str.isupper() and len([v for v in row if str(v).strip()]) <= 2\n",
    "\n",
    "print(\"ðŸ“Š Parsing cases...\")\n",
    "cases = []\n",
    "current_region = \"UNKNOWN\"\n",
    "\n",
    "for idx, row in raw.iterrows():\n",
    "    row_values = [str(c).strip() for c in row.tolist()]\n",
    "    \n",
    "    # Check if this is a section header\n",
    "    if is_section_header(row_values):\n",
    "        current_region = \" \".join([v for v in row_values if v]).strip()\n",
    "        continue\n",
    "    \n",
    "    # Skip if row is too short (less than 3 columns with data)\n",
    "    non_empty = [v for v in row_values if v and v != \"nan\"]\n",
    "    if len(non_empty) < 3:\n",
    "        continue\n",
    "    \n",
    "    # Build summary text from all fields\n",
    "    summary_text = \" \".join(non_empty)\n",
    "    \n",
    "    # Try to extract structured fields (adjust indices based on actual PDF structure)\n",
    "    case = {\n",
    "        \"region\": current_region,\n",
    "        \"raw_fields\": row_values,\n",
    "        \"summary_text\": summary_text,\n",
    "        \"case_name\": row_values[0] if len(row_values) > 0 else None,\n",
    "        \"year\": None,\n",
    "        \"court\": None,\n",
    "        \"damages\": None\n",
    "    }\n",
    "    \n",
    "    # Try to extract year (4 digits)\n",
    "    year_match = re.search(r'\\b(19|20)\\d{2}\\b', summary_text)\n",
    "    if year_match:\n",
    "        case[\"year\"] = int(year_match.group(0))\n",
    "    \n",
    "    # Try to extract dollar amounts\n",
    "    dollar_amounts = re.findall(r'\\$[\\d,]+', summary_text)\n",
    "    if dollar_amounts:\n",
    "        case[\"damages\"] = clean_currency(dollar_amounts[0])\n",
    "    \n",
    "    cases.append(case)\n",
    "\n",
    "print(f\"âœ… Parsed {len(cases)} cases\")\n",
    "print(f\"   Regions found: {len(set(c['region'] for c in cases))}\")\n",
    "print(f\"   Sample regions: {list(set(c['region'] for c in cases))[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Embeddings\n",
    "\n",
    "Create semantic embeddings for each case to enable similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ§  Generating embeddings...\")\n",
    "print(\"   This may take 1-3 minutes depending on dataset size...\")\n",
    "\n",
    "for case in tqdm(cases, desc=\"Creating embeddings\"):\n",
    "    # Combine region and summary for better semantic matching\n",
    "    text_for_embedding = f\"{case['region']} {case['summary_text']}\"\n",
    "    case[\"embedding\"] = model.encode(text_for_embedding).tolist()\n",
    "\n",
    "print(\"âœ… Embeddings generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Processed Data\n",
    "\n",
    "Save the final dataset with embeddings as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ðŸ’¾ Saving processed data to {OUTPUT_JSON}...\")\n",
    "\n",
    "with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cases, f, indent=2)\n",
    "\n",
    "print(\"âœ… Data saved successfully\")\n",
    "print(f\"\\nðŸ“Š Summary:\")\n",
    "print(f\"   Total cases: {len(cases)}\")\n",
    "print(f\"   Regions: {len(set(c['region'] for c in cases))}\")\n",
    "print(f\"   Cases with damages: {sum(1 for c in cases if c['damages'])}\")\n",
    "print(f\"   Cases with year: {sum(1 for c in cases if c['year'])}\")\n",
    "print(f\"\\nâœ… Ready to use in Streamlit app!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Quick Data Inspection\n",
    "\n",
    "View a sample of the extracted cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample cases\n",
    "print(\"\\nðŸ“‹ Sample Cases:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, case in enumerate(cases[:3], 1):\n",
    "    print(f\"\\nCase {i}:\")\n",
    "    print(f\"  Region: {case['region']}\")\n",
    "    print(f\"  Case Name: {case['case_name']}\")\n",
    "    print(f\"  Year: {case['year']}\")\n",
    "    print(f\"  Damages: ${case['damages']:,.0f}\" if case['damages'] else \"  Damages: Not found\")\n",
    "    print(f\"  Summary: {case['summary_text'][:150]}...\")\n",
    "    print(f\"  Embedding dimension: {len(case['embedding'])}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run the Streamlit app: `streamlit run streamlit_app.py`\n",
    "2. Test the search functionality with various injury descriptions\n",
    "3. Refine region mappings in `region_map.json` if needed\n",
    "4. Add custom SVG body diagrams in the `assets/` folder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
