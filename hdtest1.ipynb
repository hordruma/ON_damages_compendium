{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d6435a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 667 pages\n",
      "Sample page text:\n",
      "COMPENDIUM OF DAMAGES AWARDED IN\n",
      "PERSONAL INJURY ACTIONS ACROSS ONTARIO\n",
      "JANUARY 1999 - OCTOBER 2024\n",
      "THE HONOURABLE JUSTICE JAYE HOOPER\n",
      "AND THE HONOURABLE JAMES B. CHADWICK, Q.C.\n",
      "ANDREW CLARKE (University of Ottawa Common Law Student)\n",
      "In Acknowledgment of the Contributions of\n",
      "STEPHEN BLAIR (Sessional Professor – University of Ottawa)\n",
      "SARAH SAAD (Associate – Low Murchison Radnoff LLP)\n",
      "LIAM CARDILL (Partner – MBC Law)\n",
      "ETHAN ZAVARELLA, HANEEN FAISAL, BENJAMIN ISAAK, PHILIP BYUN, PARISA KHAZRA, ALEX DIGIOVANNI,\n",
      "CAMERON FYNNEY, JORDANA KROFT, CALEB TIMMERMANN, GWENDOLEN BOYLE, JACK KENT, ANTONIO\n",
      "GIAMBERARDINO, KATARINA GERMANI, JULIAN COSENTINO, STEPHEN LAJEUNESSE, DENNIS MYERS, DAVID\n",
      "TURNER, SEAN BAWDEN, LESLIE KIRK, CHRISTINA PARKES, MÉLANIE SICOTTE, DAWN SEARLE, ALEXANDRA\n",
      "SCHORAH, SEETHA L. RAMANATHAN, MICHELLE LUTFY AND JACK KENT\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "\n",
    "PDF_PATH = \"2024damagescompendium.pdf\"\n",
    "\n",
    "def extract_text_pdfplumber(path):\n",
    "    pages = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for i, p in enumerate(pdf.pages):\n",
    "            text = p.extract_text()\n",
    "            if not text:\n",
    "                print(f\"[warn] Page {i+1} had no extractable text (may be scanned).\")\n",
    "                text = \"\"\n",
    "            pages.append(text)\n",
    "    return pages\n",
    "\n",
    "pages = extract_text_pdfplumber(PDF_PATH)\n",
    "\n",
    "print(f\"Extracted {len(pages)} pages\")\n",
    "print(\"Sample page text:\")\n",
    "print(pages[0][:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c72f8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting pages...\n",
      "Extracted 667 pages.\n",
      "Building hierarchical JSON (TOC detection + mapping)\n",
      "Deduplicating obvious duplicates within sections...\n",
      "Writing outputs...\n",
      "Done.\n",
      "Hierarchical JSON: compendium_parsed_hier.json\n",
      "NDJSON (per-case): compendium_parsed.json\n",
      "CSV preview: compendium_parsed_preview.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "parse_compendium_hierarchical.py\n",
    "\n",
    "Produces:\n",
    "  - /mnt/data/compendium_parsed_hier.json   (single hierarchical JSON object, pretty-printed)\n",
    "  - /mnt/data/compendium_parsed.json        (newline-delimited JSON: one case per line)\n",
    "  - /mnt/data/compendium_parsed_preview.csv (CSV preview)\n",
    "\n",
    "Assumptions & approach:\n",
    "  - Uses pdfplumber only (install via pip).\n",
    "  - Reads the Table of Contents area (searching first ~10 pages) to build section names + printed page numbers.\n",
    "  - Maps section names to actual PDF page indices by searching for the section heading text in extracted pages.\n",
    "  - Splits each section into blocks using the appearance of the \"table header\" row or blank-line heuristics.\n",
    "  - Extracts fields with conservative regexes and then structures records hierarchically under sections.\n",
    "  - Breaks multi-injury rows into multiple injury entries nested under the same case.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import pdfplumber\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "PDF_PATH = \"2024damagescompendium.pdf\"\n",
    "OUT_HIER_JSON = \"compendium_parsed_hier.json\"\n",
    "OUT_NDJSON = \"compendium_parsed.json\"\n",
    "OUT_CSV = \"compendium_parsed_preview.csv\"\n",
    "\n",
    "# ---------- Utilities for extraction/parsing ----------\n",
    "\n",
    "def extract_pages_text(path: Path) -> List[str]:\n",
    "    \"\"\"Extract text from each page using pdfplumber; returns list of page texts (index 0 => page 1).\"\"\"\n",
    "    pages = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for i, p in enumerate(pdf.pages):\n",
    "            text = p.extract_text() or \"\"\n",
    "            pages.append(text)\n",
    "    return pages\n",
    "\n",
    "def find_toc_block(pages: List[str], toc_anchor=\"TABLE OF CONTENTS\", search_pages=12) -> Optional[Tuple[int,str]]:\n",
    "    \"\"\"Return (page_index, toc_text) for the first page containing 'TABLE OF CONTENTS' within search_pages.\"\"\"\n",
    "    for i in range(min(search_pages, len(pages))):\n",
    "        if pages[i] and toc_anchor.lower() in pages[i].lower():\n",
    "            # capture a few pages after in case TOC spans multiple pages\n",
    "            combined = pages[i]\n",
    "            for j in range(i+1, min(len(pages), i+6)):\n",
    "                # include subsequent pages as long as they look like TOC (many dots or digits)\n",
    "                if re.search(r'\\.{10,}', pages[j]) or re.search(r'\\b\\d{1,3}\\b', pages[j]):\n",
    "                    combined += \"\\n\\n\" + pages[j]\n",
    "                else:\n",
    "                    break\n",
    "            return i, combined\n",
    "    return None\n",
    "\n",
    "def parse_toc_lines(toc_text: str) -> List[Tuple[str, Optional[int]]]:\n",
    "    \"\"\"\n",
    "    Parse TOC lines into (section_name, printed_page_number).\n",
    "    Heuristic: lines with many dots or long whitespace then a number.\n",
    "    \"\"\"\n",
    "    lines = [l.strip() for l in toc_text.splitlines() if l.strip()]\n",
    "    parsed = []\n",
    "    for ln in lines:\n",
    "        # common TOC line: SECTION NAME ..... 123\n",
    "        m = re.match(r'^(?P<section>.+?)\\s+\\.{3,}\\s*(?P<pnum>\\d{1,4})\\s*$', ln)\n",
    "        if not m:\n",
    "            # alternative: words then many spaces then number\n",
    "            m = re.match(r'^(?P<section>.+?)\\s{2,}(?P<pnum>\\d{1,4})\\s*$', ln)\n",
    "        if m:\n",
    "            name = re.sub(r'\\s+', ' ', m.group('section')).strip()\n",
    "            parsed.append((name, int(m.group('pnum'))))\n",
    "        else:\n",
    "            # also capture lines which are all caps and likely a section header with no page number on same line\n",
    "            if ln.isupper() and len(ln.split()) <= 6:\n",
    "                parsed.append((ln, None))\n",
    "    # Deduplicate and clean\n",
    "    cleaned = []\n",
    "    seen = set()\n",
    "    for name, pnum in parsed:\n",
    "        key = name.lower()\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        cleaned.append((name, pnum))\n",
    "    return cleaned\n",
    "\n",
    "def map_sections_to_pages(sections: List[Tuple[str,Optional[int]]], pages: List[str]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    For each TOC section try to find the first page index where that section name appears.\n",
    "    Fallback: if printed page number exists, attempt to map via that (printed page number -> index = printed-1).\n",
    "    Returns list of dicts: {'name','printed_page','page_index'}.\n",
    "    \"\"\"\n",
    "    mapped = []\n",
    "    n_pages = len(pages)\n",
    "    for name, printed in sections:\n",
    "        lower_name = re.sub(r'[^a-z0-9 ]', '', name.lower())\n",
    "        found_index = None\n",
    "        # search pages for the name as substring (case-insensitive)\n",
    "        for i, ptxt in enumerate(pages):\n",
    "            if not ptxt:\n",
    "                continue\n",
    "            if lower_name and lower_name in re.sub(r'[^a-z0-9 ]','', ptxt.lower()):\n",
    "                found_index = i\n",
    "                break\n",
    "        if found_index is None and printed is not None:\n",
    "            candidate = printed - 1\n",
    "            if 0 <= candidate < n_pages:\n",
    "                found_index = candidate\n",
    "        # if still None, set to 0 (document fallback)\n",
    "        if found_index is None:\n",
    "            found_index = 0\n",
    "        mapped.append({'name': name, 'printed_page': printed, 'page_index': found_index})\n",
    "    # sort by page_index\n",
    "    mapped.sort(key=lambda x: x['page_index'])\n",
    "    return mapped\n",
    "\n",
    "# ---------- Block splitting and field extraction ----------\n",
    "\n",
    "TABLE_HEADER_RE = re.compile(r'Plaintiff\\s+Defendant\\s+Year\\s+Citation\\s+Court\\s+Judge\\s+Sex', re.I)\n",
    "\n",
    "def split_section_into_blocks(section_pages_text: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Join pages and split into candidate blocks representing individual cases.\n",
    "    Prefer splitting where the table header appears; otherwise use blank-line heuristics.\n",
    "    \"\"\"\n",
    "    joined = \"\\n\\n\".join(section_pages_text)\n",
    "    # If the table header appears, split using it as anchor (slice into rows after header)\n",
    "    if TABLE_HEADER_RE.search(joined):\n",
    "        # remove header lines so we can split into records by occurrence of a year/case name pattern\n",
    "        # attempt to split by blank-lines where a new case probably starts (line starting with uppercase name)\n",
    "        parts = re.split(r'\\n\\s*\\n', joined)\n",
    "        # group contiguous lines into blocks where a line looks like a case heading (e.g., starts with a name)\n",
    "        blocks = []\n",
    "        cur = []\n",
    "        for p in parts:\n",
    "            # treat lines starting with a capitalized word and containing comma or colon or ending with 'J.' as possible start\n",
    "            first_line = p.splitlines()[0].strip() if p.strip() else ''\n",
    "            if re.match(r'^[A-Z][A-Za-z0-9\\-\\&\\.\\' ]{2,50}(:|,|\\sJ\\.|\\sS\\.C\\.J\\.|\\sS\\.C\\.J:)?', first_line):\n",
    "                if cur:\n",
    "                    blocks.append(\"\\n\\n\".join(cur))\n",
    "                cur = [p]\n",
    "            else:\n",
    "                cur.append(p)\n",
    "        if cur:\n",
    "            blocks.append(\"\\n\\n\".join(cur))\n",
    "        # filter too-short items\n",
    "        blocks = [b.strip() for b in blocks if len(b.strip()) > 60]\n",
    "        return blocks\n",
    "    # fallback: split by double newlines and then join into roughly-case-sized chunks\n",
    "    parts = [p.strip() for p in joined.split('\\n\\n') if p.strip()]\n",
    "    blocks = []\n",
    "    cur = []\n",
    "    for p in parts:\n",
    "        cur.append(p)\n",
    "        if len(\"\\n\".join(cur)) > 300:\n",
    "            blocks.append(\"\\n\".join(cur))\n",
    "            cur = []\n",
    "    if cur:\n",
    "        blocks.append(\"\\n\".join(cur))\n",
    "    return blocks\n",
    "\n",
    "INJURY_KEYWORDS = [\n",
    "    \"brain\",\"skull\",\"head\",\"neck\",\"spine\",\"back\",\"whiplash\",\"arm\",\"wrist\",\"hand\",\"elbow\",\n",
    "    \"shoulder\",\"hip\",\"knee\",\"leg\",\"ankle\",\"foot\",\"eye\",\"ear\",\"tooth\",\"dental\",\"scar\",\n",
    "    \"burn\",\"laceration\",\"concussion\",\"paraplegia\",\"quadriplegia\",\"nerve\",\"psych\", \"depression\"\n",
    "]\n",
    "\n",
    "def extract_fields_from_block(block: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Conservative field extraction producing a structured dict for a case block.\n",
    "    Fields: case_name, year, citation, court, judge, sex, age, values (list),\n",
    "            non_pecuniary, general_damages, other_damages, comments,\n",
    "            injuries (list of dicts), raw.\n",
    "    Each injury dict = {'text': ..., 'regions': [...], 'values': [...]}\n",
    "    \"\"\"\n",
    "    out = {\n",
    "        'case_name': None,\n",
    "        'year': None,\n",
    "        'citation': None,\n",
    "        'court': None,\n",
    "        'judge': None,\n",
    "        'sex': None,\n",
    "        'age': None,\n",
    "        'values': [],\n",
    "        'non_pecuniary': None,\n",
    "        'general_damages': None,\n",
    "        'other_damages': None,\n",
    "        'comments': None,\n",
    "        'injuries': [],\n",
    "        'raw': block\n",
    "    }\n",
    "\n",
    "    # Year: first 4-digit year within plausible range\n",
    "    ym = re.search(r'\\b(19|20)\\d{2}\\b', block)\n",
    "    if ym:\n",
    "        out['year'] = ym.group(0)\n",
    "\n",
    "    # Candidate case name: line that seems like the first heading\n",
    "    lines = [l.strip() for l in block.splitlines() if l.strip()]\n",
    "    if lines:\n",
    "        # often the first meaningful long line is the case title\n",
    "        out['case_name'] = lines[0][:240]\n",
    "\n",
    "    # All $ amounts found\n",
    "    vals = re.findall(r'\\$[\\d,]+(?:\\.\\d{2})?', block)\n",
    "    out['values'] = vals\n",
    "\n",
    "    # General / Non-Pecuniary\n",
    "    g = re.search(r'General\\s*Damages?\\s*[:\\-]?\\s*(\\$[\\d,]+(?:\\.\\d{2})?)', block, re.I)\n",
    "    if g:\n",
    "        out['general_damages'] = g.group(1)\n",
    "    np = re.search(r'Non[- ]?Pecuniary[\\s\\S]{0,120}(\\$[\\d,]+(?:\\.\\d{2})?)', block, re.I)\n",
    "    if np:\n",
    "        out['non_pecuniary'] = np.group(1)\n",
    "\n",
    "    # Comments: after 'Comments' label or last long paragraph\n",
    "    if 'Comments' in block:\n",
    "        out['comments'] = block.split('Comments',1)[1].strip()\n",
    "    else:\n",
    "        paras = [p for p in re.split(r'\\n\\s*\\n', block) if len(p.strip()) > 40]\n",
    "        out['comments'] = paras[-1].strip() if paras else ''\n",
    "\n",
    "    # Detect injuries: extract sentences/clauses that contain keywords\n",
    "    sentences = re.split(r'[;\\.\\n]\\s+', out['comments']) if out['comments'] else []\n",
    "    injuries_found = []\n",
    "    for s in sentences:\n",
    "        for kw in INJURY_KEYWORDS:\n",
    "            if re.search(r'\\b' + re.escape(kw) + r'\\b', s, re.I):\n",
    "                injuries_found.append(s.strip())\n",
    "                break\n",
    "\n",
    "    # If none found in comments, search entire block and capture snippets\n",
    "    if not injuries_found:\n",
    "        for kw in INJURY_KEYWORDS:\n",
    "            for m in re.finditer(r'(.{0,80}\\b' + re.escape(kw) + r'\\b.{0,80})', block, re.I):\n",
    "                injuries_found.append(m.group(1).strip())\n",
    "        # dedupe\n",
    "        injuries_found = list(dict.fromkeys(injuries_found))\n",
    "\n",
    "    # Convert each injury snippet to a structured dict\n",
    "    injuries_structured = []\n",
    "    for inj in injuries_found:\n",
    "        regs = sorted(set([kw for kw in INJURY_KEYWORDS if re.search(r'\\b' + re.escape(kw) + r'\\b', inj, re.I)]))\n",
    "        vals_inj = re.findall(r'\\$[\\d,]+(?:\\.\\d{2})?', inj)\n",
    "        injuries_structured.append({'text': inj, 'regions': regs, 'values': vals_inj})\n",
    "\n",
    "    out['injuries'] = injuries_structured\n",
    "\n",
    "    return out\n",
    "\n",
    "# ---------- High-level orchestration ----------\n",
    "\n",
    "def build_hierarchical_json(pages_text: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Build hierarchical JSON object:\n",
    "    {\n",
    "      'title': <first page heading or file name>,\n",
    "      'sections': [\n",
    "         {\n",
    "           'name': 'HEAD',\n",
    "           'printed_page': 1,\n",
    "           'page_index': 3,\n",
    "           'cases': [ {case dict}, ... ]\n",
    "         }, ...\n",
    "      ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    title = None\n",
    "    if pages_text and pages_text[0]:\n",
    "        # first non-empty line on first page\n",
    "        first_lines = [l.strip() for l in pages_text[0].splitlines() if l.strip()]\n",
    "        if first_lines:\n",
    "            title = first_lines[0]\n",
    "\n",
    "    # 1) Find TOC and parse\n",
    "    toc_info = find_toc_block(pages_text, toc_anchor=\"TABLE OF CONTENTS\", search_pages=12)\n",
    "    if toc_info:\n",
    "        toc_page_idx, toc_text = toc_info\n",
    "        sections = parse_toc_lines(toc_text)\n",
    "    else:\n",
    "        # If no TOC, fall back to top-level all-caps headings found in the first ~50 pages\n",
    "        sections = []\n",
    "        for i in range(min(50, len(pages_text))):\n",
    "            txt = pages_text[i] or \"\"\n",
    "            for line in txt.splitlines():\n",
    "                ln = line.strip()\n",
    "                if ln.isupper() and len(ln) > 3 and len(ln.split()) <= 6:\n",
    "                    sections.append((ln, None))\n",
    "        # fallback unique\n",
    "        seen = set()\n",
    "        sections = [s for s in sections if not (s[0].lower() in seen or seen.add(s[0].lower()))]\n",
    "\n",
    "    # 2) Map to actual page indices\n",
    "    mapped = map_sections_to_pages(sections, pages_text)\n",
    "    # append sentinel end section at end of document\n",
    "    for i, m in enumerate(mapped):\n",
    "        mapped[i]['cases'] = []\n",
    "\n",
    "    # 3) Determine page ranges for each mapped section\n",
    "    for i in range(len(mapped)):\n",
    "        start = mapped[i]['page_index']\n",
    "        end = mapped[i+1]['page_index'] if i+1 < len(mapped) else len(pages_text)\n",
    "        # collect page texts for this section\n",
    "        sec_pages = [pages_text[p] or \"\" for p in range(start, end)]\n",
    "        # 4) split section into blocks\n",
    "        blocks = split_section_into_blocks(sec_pages)\n",
    "        # 5) parse blocks into structured cases\n",
    "        for b in blocks:\n",
    "            case = extract_fields_from_block(b)\n",
    "            # case-level duplicate detection may be applied downstream\n",
    "            mapped[i]['cases'].append(case)\n",
    "\n",
    "    # 6) Build top-level JSON\n",
    "    result = {\n",
    "        'title': title or PDF_PATH.name,\n",
    "        'source_pdf_path': str(PDF_PATH),\n",
    "        'sections': mapped\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# ---------- Write outputs ----------\n",
    "\n",
    "def dedupe_cases_in_sections(hier: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Optionally deduplicate identical case raw texts within each section (exact raw match).\n",
    "    Keeps first occurrence.\n",
    "    \"\"\"\n",
    "    for sec in hier.get('sections', []):\n",
    "        seen = set()\n",
    "        new_cases = []\n",
    "        for c in sec.get('cases', []):\n",
    "            r = c.get('raw', '')\n",
    "            key = r.strip()[:150]  # a short fingerprint\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            new_cases.append(c)\n",
    "        sec['cases'] = new_cases\n",
    "    return hier\n",
    "\n",
    "def write_outputs(hier: Dict):\n",
    "    # pretty hierarchical JSON\n",
    "    with open(OUT_HIER_JSON, 'w', encoding='utf-8') as f:\n",
    "        json.dump(hier, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # NDJSON (one case per line) plus section metadata\n",
    "    with open(OUT_NDJSON, 'w', encoding='utf-8') as f:\n",
    "        for sec in hier.get('sections', []):\n",
    "            for case in sec.get('cases', []):\n",
    "                out = dict(case)  # shallow copy\n",
    "                out['_section'] = sec['name']\n",
    "                out['_section_printed_page'] = sec.get('printed_page')\n",
    "                out['_section_page_index'] = sec.get('page_index')\n",
    "                f.write(json.dumps(out, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    # CSV preview\n",
    "    preview_cols = ['_section','case_name','year','general_damages','non_pecuniary','values','injuries_text','injury_regions']\n",
    "    with open(OUT_CSV, 'w', newline='', encoding='utf-8') as csvf:\n",
    "        w = csv.writer(csvf)\n",
    "        w.writerow(preview_cols)\n",
    "        for sec in hier.get('sections', []):\n",
    "            for c in sec.get('cases', []):\n",
    "                injuries_text = \" | \".join([i['text'] for i in c.get('injuries', [])]) if c.get('injuries') else ''\n",
    "                injury_regions = \"; \".join(sorted({r for i in c.get('injuries',[]) for r in i.get('regions',[]) })) if c.get('injuries') else ''\n",
    "                row = [\n",
    "                    sec['name'],\n",
    "                    c.get('case_name'),\n",
    "                    c.get('year'),\n",
    "                    c.get('general_damages'),\n",
    "                    c.get('non_pecuniary'),\n",
    "                    \"; \".join(c.get('values',[])),\n",
    "                    injuries_text,\n",
    "                    injury_regions\n",
    "                ]\n",
    "                w.writerow(row)\n",
    "\n",
    "# ---------- Main ----------\n",
    "\n",
    "\n",
    "\n",
    "print(\"Extracting pages...\")\n",
    "pages = extract_pages_text(PDF_PATH)\n",
    "print(f\"Extracted {len(pages)} pages.\")\n",
    "\n",
    "print(\"Building hierarchical JSON (TOC detection + mapping)\")\n",
    "hier = build_hierarchical_json(pages)\n",
    "\n",
    "print(\"Deduplicating obvious duplicates within sections...\")\n",
    "hier = dedupe_cases_in_sections(hier)\n",
    "\n",
    "print(\"Writing outputs...\")\n",
    "write_outputs(hier)\n",
    "\n",
    "print(\"Done.\")\n",
    "print(f\"Hierarchical JSON: {OUT_HIER_JSON}\")\n",
    "print(f\"NDJSON (per-case): {OUT_NDJSON}\")\n",
    "print(f\"CSV preview: {OUT_CSV}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
